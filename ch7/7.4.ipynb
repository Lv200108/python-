{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 案例应用\n",
    "\n",
    "### 7.4.1 MNIST手写体识别\n",
    "\n",
    "【例7-14】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d3347f1b5f0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 超参数设置\n",
    "    BATCH_SIZE = 64  # 批次大小\n",
    "    EPOCHS = 10  # 迭代次数\n",
    "    learning_rate = 0.01  # 学习率\n",
    "    # 如果cuda可用，则使用cuda，否则使用CPU训练\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # 实例化LeNet5模型\n",
    "    lenet5 = LeNet5().to(DEVICE)\n",
    "    # 实例化交叉熵损失函数（包含了softmax损失函数）\n",
    "    criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "    # 实例化Adam优化器\n",
    "    optimizer = optim.Adam(lenet5.parameters(), lr=learning_rate)\n",
    "    # 获取训练集数据加载器\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train=True, download=True, transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize(mean=(0.5,), std=(0.5,))])),\n",
    "        batch_size=BATCH_SIZE, shuffle=True)\n",
    "    # 获取测试集数据加载器\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=True)\n",
    "    # 训练模型\n",
    "    print('---------训练模型---------')\n",
    "    train(EPOCHS, lenet5, criterion, DEVICE, train_loader, optimizer)\n",
    "    # 测试模型\n",
    "    print('---------测试模型---------')\n",
    "    test(test_loader, lenet5, criterion, DEVICE)\n",
    "\n",
    "\n",
    "# 定义AlexNet模型\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        # 搭建由卷积层、池化层和激活函数组成的特征提取器\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(2, 2),\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(2, 2),\n",
    "        )\n",
    "        # 创建分类器\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16 * 4 * 4, 120),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Linear(84, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        # 平铺特征图\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(num_epochs, model, criterion, device, train_loader, optimizer):\n",
    "    for epoch in range(num_epochs):\n",
    "        # 设置模型为训练模式\n",
    "        model.train()\n",
    "        # test_loss为训练时的总损失\n",
    "        total_test_loss = 0\n",
    "        # 从迭代器抽取图片和标签\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            samples,labels = images.to(device),labels.to(device)\n",
    "            # 获取当前模型的输出\n",
    "            output = model(samples.reshape(-1, 1, 28, 28))\n",
    "            # 计算损失函数值\n",
    "            loss = criterion(output, labels)\n",
    "            # 参数更新前优化器内部参数梯度设置为0\n",
    "            optimizer.zero_grad()\n",
    "            # 损失值前向传播\n",
    "            loss.backward()\n",
    "            # 更新模型参数\n",
    "            optimizer.step()\n",
    "            # 转成Python数据类型\n",
    "            loss = loss.item()\n",
    "            # 累计损失\n",
    "            total_test_loss += loss\n",
    "        model.eval()  # 设置模型进入预测模式\n",
    "        correct = 0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # 获得模型输出\n",
    "            output = model(data.reshape(-1, 1, 28, 28))\n",
    "            # 获得模型分类结果\n",
    "            pred = output.data.max(1)[1]\n",
    "            # 计算分类正确的样本数\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        # 打印迭代训练过程中训练集的损失和准确率\n",
    "        print(\"Epoch: {}/{}\".format(epoch + 1, num_epochs))\n",
    "        print(\"Train-lose: {:.4f}\".format(total_test_loss / len(train_loader.dataset)))\n",
    "        print('Accuracy: {:.3f}%\\n'.format(100. * correct / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(test_loader, model, criterion, device):\n",
    "    # 设置模型为验证模式\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data.reshape(-1, 1, 28, 28))\n",
    "        # 找到概率最大的下标，为输出值\n",
    "        pred = output.data.max(1)[1]\n",
    "        # 正确的样本数\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "    # 打印测试结果\n",
    "    print('Total number of test samples: ', len(test_loader.dataset))\n",
    "    print('Number of correct classifications: ', correct)\n",
    "    print('Accuracy: {:.3f}% '.format(100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2 猫狗大战\n",
    "\n",
    "【例7-16】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\llh\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:318: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------训练模型---------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8e24f3a0cb94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[0mdata_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m     \u001b[0mmain2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-8e24f3a0cb94>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;31m# 训练模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'---------训练模型---------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlenet5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m     \u001b[1;31m# 测试模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'---------测试模型---------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-8e24f3a0cb94>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(num_epochs, model, criterion, device, train_loader, optimizer)\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[1;31m# 损失值前向传播\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m             \u001b[1;31m# 更新模型参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "# 制作数据集\n",
    "def data_preprocess():\n",
    "    # kaggle原始数据中‘train’的地址\n",
    "    original_dataset_dir = 'train'\n",
    "    total_num = int(len(os.listdir(original_dataset_dir)) / 2)\n",
    "    index = np.array(range(total_num))\n",
    "    # 待处理的数据集地址\n",
    "    base_dir = 'cats_and_dogs'\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.mkdir(base_dir)\n",
    "    # 训练集、测试集的划分\n",
    "    sub_dirs = ['train', 'test']\n",
    "    animals = ['cats', 'dogs']\n",
    "    train_index = index[:int(total_num * 0.9)]\n",
    "    test_index = index[int(total_num * 0.9):]\n",
    "    numbers = [train_index, test_index]\n",
    "    for idx, sub_dir in enumerate(sub_dirs):\n",
    "        dir = os.path.join(base_dir, sub_dir)\n",
    "        if not os.path.exists(dir):\n",
    "            os.mkdir(dir)\n",
    "        for animal in animals:\n",
    "            animal_dir = os.path.join(dir, animal)\n",
    "            if not os.path.exists(animal_dir):\n",
    "                os.mkdir(animal_dir)\n",
    "            fnames = [animal[:-1] + '.{}.jpg'.format(i) for i in numbers[idx]]\n",
    "            for fname in fnames:\n",
    "                src = os.path.join(original_dataset_dir, fname)\n",
    "                dst = os.path.join(animal_dir, fname)\n",
    "                shutil.copyfile(src, dst)\n",
    "            # 验证训练集、测试集的划分的照片数目\n",
    "            print(animal_dir + ' total images : %d' % (len(os.listdir(animal_dir))))\n",
    "\n",
    "\n",
    "def get_dataloader(train=True, root='cats_and_dogs/train/', batch_size=8):\n",
    "    loader = None\n",
    "    dataset = None\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    if train:\n",
    "        dataset = datasets.ImageFolder(root=root, transform=data_transform)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, )\n",
    "    else:\n",
    "        dataset = datasets.ImageFolder(root=root, transform=data_transform)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, )\n",
    "    return loader\n",
    "\n",
    "\n",
    "# 创建模型\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        # 搭建由卷积层、池化层和激活函数组成的特征提取器\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(2, 2),\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(2, 2),\n",
    "        )\n",
    "        # 创建分类器\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(44944, 120),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Linear(84, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        # 平铺特征图\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 使用LeNet-5进行猫狗预测\n",
    "def main():\n",
    "    # 超参数设置\n",
    "    BATCH_SIZE = 256  # 批次大小\n",
    "    EPOCHS = 10  # 迭代次数\n",
    "    learning_rate = 0.01  # 学习率\n",
    "    # 如果cuda可用，则使用cuda，否则使用CPU训练\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # 实例化LeNet5模型\n",
    "    lenet5 = LeNet5().to(DEVICE)\n",
    "    # 实例化交叉熵损失函数（包含了softmax损失函数）\n",
    "    criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "    # 实例化Adam优化器\n",
    "    optimizer = optim.Adam(lenet5.parameters(), lr=learning_rate)\n",
    "    # 获取训练集数据加载器\n",
    "    train_loader = get_dataloader(train=True, root='cats_and_dogs/train/', batch_size=BATCH_SIZE)\n",
    "    # 获取测试集数据加载器\n",
    "    test_loader = get_dataloader(train=False, root='cats_and_dogs/test/', batch_size=BATCH_SIZE)\n",
    "    # 训练模型\n",
    "    print('---------训练模型---------')\n",
    "    train(EPOCHS, lenet5, criterion, DEVICE, train_loader, optimizer)\n",
    "    # 测试模型\n",
    "    print('---------测试模型---------')\n",
    "    test(test_loader, lenet5, criterion, DEVICE)\n",
    "\n",
    "\n",
    "# ResNet迁移学习\n",
    "def main2():\n",
    "    # 超参数设置\n",
    "    BATCH_SIZE = 16  # 批次大小\n",
    "    EPOCHS = 10  # 迭代次数\n",
    "    learning_rate = 0.01  # 学习率\n",
    "    # 如果cuda可用，则使用cuda，否则使用CPU训练\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # 使用预训练的resnet50，进行基于模型的迁移\n",
    "    resnet = models.resnet50(pretrained=True).to(DEVICE)\n",
    "    # 实例化交叉熵损失函数（包含了softmax损失函数）\n",
    "    criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "    # 实例化Adam优化器\n",
    "    optimizer = optim.Adam(resnet.parameters(), lr=learning_rate)\n",
    "    # 获取训练集数据加载器\n",
    "    train_loader = get_dataloader(train=True, root='cats_and_dogs/train/', batch_size=BATCH_SIZE)\n",
    "    # 获取测试集数据加载器\n",
    "    test_loader = get_dataloader(train=False, root='cats_and_dogs/test/', batch_size=BATCH_SIZE)\n",
    "    # 训练模型\n",
    "    print('---------训练模型---------')\n",
    "    train(EPOCHS, resnet, criterion, DEVICE, train_loader, optimizer)\n",
    "    # 测试模型\n",
    "    print('---------测试模型---------')\n",
    "    test(test_loader, resnet, criterion, DEVICE)\n",
    "\n",
    "\n",
    "# 训练\n",
    "def train(num_epochs, model, criterion, device, train_loader, optimizer):\n",
    "    for epoch in range(num_epochs):\n",
    "        # 设置模型为训练模式\n",
    "        model.train()\n",
    "        # test_loss为训练时的总损失\n",
    "        total_test_loss = 0\n",
    "        # 从迭代器抽取图片和标签\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            samples = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # 获取当前模型的输出\n",
    "            output = model(samples)\n",
    "            # 计算损失函数值\n",
    "            loss = criterion(output, labels)\n",
    "            # 参数更新前优化器内部参数梯度设置为0\n",
    "            optimizer.zero_grad()\n",
    "            # 损失值前向传播\n",
    "            loss.backward()\n",
    "            # 更新模型参数\n",
    "            optimizer.step()\n",
    "            # 转成Python数据类型\n",
    "            loss = loss.item()\n",
    "            # 累计损失\n",
    "            total_test_loss += loss\n",
    "        model.eval()  # 设置模型进入预测模式\n",
    "        correct = 0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # 获得模型输出\n",
    "            output = model(data)\n",
    "            # 获得模型分类结果\n",
    "            pred = output.data.max(1)[1]\n",
    "            # 计算分类正确的样本数\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        # 打印迭代训练过程中训练集的损失和准确率\n",
    "        print(\"Epoch: {}/{}\".format(epoch + 1, num_epochs))\n",
    "        print(\"Train-lose: {:.4f}\".format(total_test_loss / len(train_loader.dataset)))\n",
    "        print('Accuracy: {:.3f}%\\n'.format(100. * correct / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "# 测试\n",
    "def test(test_loader, model, criterion, device):\n",
    "    # 设置模型为验证模式\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        # 找到概率最大的下标，为输出值\n",
    "        pred = output.data.max(1)[1]\n",
    "        # 正确的样本数\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "    # 打印测试结果\n",
    "    print('Total number of test samples: ', len(test_loader.dataset))\n",
    "    print('Number of correct classifications: ', correct)\n",
    "    print('Accuracy: {:.3f}% '.format(100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists('cats_and_dogs'):\n",
    "        data_preprocess()\n",
    "    main()\n",
    "    main2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
